<p><link rel="stylesheet" type="text/css" href="markdown.css" />
<!-- To generate the html from this markdown file: perl ~/Markdown.pl --html4tags README.md > static/README.html --></p>

<h1>perfboard</h1>

<p>A performance metrics dashboard for continuous context recognition systems</p>

<h2>Background</h2>

<p>This project provides a framework for the evaluation and visualization of continuous context-recognition systems. We are investigating systems for automatic detection of visits, paths, and activities from mobile sensor data, and develop these tools to help evaluate the feasibility and quality of various approaches.</p>

<h2>Overview</h2>

<p>This section provides an overview of a typical development life-cycle for continuous context recognition systems and explains how this framework fits in.</p>

<ol>
<li><p><strong>Ground truth collection</strong> - Ground truth corpora consist of <a href="#raw_data">raw data</a> files, typically collected from sensors carried or worn by a subject, and <a href="#ground_truth">ground truth</a> labels provided by researchers, subjects, click workers, or other observers, over intervals of the data. The labels provide the <em>truth</em> about what the user (or device) was doing during the labeled interval, and are required for development and evaluation of most classification/detection/recognition systems.</p></li>
<li><p><strong>Recognizer development</strong> - The raw data and associated ground truth labels collected in the previous step are invaluable for the development, training, and testing of various components in a recognition system pipeline. Here we mainly focus on <a href="http://en.wikipedia.org/wiki/Binary_classification">binary classification</a> and define an abstract <a href="#recognizer_interface">recognizer interface</a> that allows recognizers to be plugged in to the system.</p></li>
<li><p><strong>Evaluation</strong> - Now, using this framework we can feed <a href="#raw_data">raw data</a> from selected <a href="#ground_truth">ground truth</a> cases to one or more recognizers that implement the <a href="#recognizer_interface">recognizer interface</a> and analyze the returned results. Results from each recognizer are compared with the labels from each ground truth case, and both individual and aggregate <a href="#test_results">test results</a> are computed and logged. Finally, detailed performance metrics are rendered for further inspection. <span class="bibref">[<a href="#ref1">1</a>]</span></p></li>
<li><p><strong>Iteration</strong> - Based on the performance results, we may return to step 2 and tweak parameters, modify code, try something new, or to step 1 to scale-up the testing, gather more test data, etc, and improve performance. Once the performance of a recognizer is above the desired level for a diverse data set, we can be more confident in the feasibility of the approach.</p></li>
</ol>

<h2>Running Performance Tests</h2>

<p>The <code>perfboard</code> command line utility can be used to test the performance for one or more classifiers using a set of labeled <a href="#ground_truth">ground truth</a> data. The tool can instantiate one or more recognizers, read a set of ground truth files, feed raw data from the ground truths to the recognizers, retrieve the results from the recognizers, and finally compute and render performance metrics. Usage:</p>

<pre><code>perfboard.py [-h] [--outpath OUTPUT_PATH] --recognizers RECOGNIZER_LIST
                    TRUTH_FILE [TRUTH_FILE ...]
</code></pre>

<p>For example, to test 3 different recognizers using the <code>example_truth.json</code> case, and write the final scored results to <code>static/scores.json</code>:</p>

<pre><code>./perfboard.py --recognizers=test.recognizers.DummyWalkingDetector,test.recognizers.DummyRunningDetector,test.recognizers.DummyStandingDetector 
               --outpath=static/scores.json  test/example_truth.json
</code></pre>

<p>Results written to  <code>static/scores.json</code> are picked up by the performance dashboard which can be accessed by running a web server serving <code>perfboard/static</code> dir, then browsing to that location, ie:</p>

<pre><code>http://localhost/perfboard/
</code></pre>

<p><a id="raw_data"/></p>

<h2>Raw Data</h2>

<p>Raw data files contain raw sensor and/or behavioral data, for example accelerometer continuously logged from the mobile device of a subject while the subject performs some sequence predefined activities. Raw data can be collected in any format, be we present a generic record structure here for various kinds of mobile data.</p>

<pre><code>coming...
</code></pre>

<p><a id="ground_truth"/></p>

<h2>Ground Truth</h2>

<p>Ground truth files encode labels containing the precise start and end times of activities performed by the user, as well as references to the <a href="#raw_data">raw data</a> that corresponds to the activities. The ground truth label files are json encoded in <a href="http://www.json.org/">json</a> and have the following form:</p>

<pre><code>{
"data_path": "raw_data/device1_20120516", 
"t1": "2012-05-16T09:00:00-08:00", 
"t2": "2012-05-16T09:20:00-08:00",
"description": "Activity recognition data collection with subject DR on morning of 5/16/2012", 
"device": "Nokia C7-00", 
"hw": "353755043225509", 
"sw": "qt-hubris-client-v0.0.3", 
"subject": {
    "bday": "08/22/1975", 
    "gender": "male", 
    "handedness": "RIGHT", 
    "height": "183cm", 
    "id": "DR", 
    "weight": "165lbs"
}, 
"labels": [
    {
    "body_position": "RIGHT_FRONT_POCKET",
    "t1": "2012-05-16T09:00:00-08:00", 
    "t2": "2012-05-20T09:05:00-08:00", 
    "label": "STANDING"
    }, 
    {
    "body_position": "RIGHT_FRONT_POCKET",
    "t1": "2012-05-16T09:05:00-08:00", 
    "t2": "2012-05-20T09:15:00-08:00", 
    "label": "WALKING"
    }, 
    {
    "body_position": "RIGHT_HAND",
    "t1": "2012-05-16T09:15:00-08:00", 
    "t2": "2012-05-20T09:15:30-08:00", 
    "label": "STANDING"
    }, 
    {
        "body_position": "RIGHT_BACK_POCKET",
    "t1": "2012-05-16T09:15:30-08:00", 
    "t2": "2012-05-20T09:18:30-08:00", 
    "label": "RUNNING"
    }, 
    {
    "body_position": "RIGHT_HAND",
    "t1": "2012-05-16T09:18:30-08:00", 
    "t2": "2012-05-20T09:20:00-08:00", 
    "label": "STANDING"
    }
], 
}
</code></pre>

<p><a id="recognizer_interface"/></p>

<h2>Recognizer Interface</h2>

<p>Recognizers process time-ordered chunks of <a href="#raw_data">raw data</a> and produce sets of <a href="#recognizer_results">recognizer results</a>. The <code>perfboard.AbstractRecognizer</code> <a href="http://python.org">python</a> class is shown here: </p>

<pre><code>class AbstractRecognizer( object ):
    """Defines a generic recognizer interface. Override `process` and `results`. 
    Set `self.labels_recognized` to specify the labels this recognizer can produce
    """
    def __init__(self):
        self.labels_recognized = []

    def process(self, chunk):
        """process chunk of data (typically json-encoded list of records) and update internal state"""
        pass

    def detected(self):
        """returned order list of recognition results."""
        pass

    def version(self):
        head, tail = os.path.split(__file__)
        module = tail.rstrip(".pyc")
        return ("%s.%s-%s"% (module, self.__class__.__name__, commands.getoutput('git rev-parse HEAD'))).lstrip("./")
</code></pre>

<p><a id="recognizer_results"/></p>

<h2>Recognizer results</h2>

<p>Recognizers output labeled time intervals of the form:</p>

<pre><code>[
    {
        "label": "STANDING", 
        "t1": "2012-05-16T09:00:00-08:00", 
        "t2": "2012-05-20T09:05:00-08:00"
    }, 
    {
        "label": "WALKING", 
        "t1": "2012-05-16T09:05:00-08:00", 
        "t2": "2012-05-20T09:15:00-08:00"
    }, 
    {
        "label": "STANDING", 
        "t1": "2012-05-16T09:15:00-08:00", 
        "t2": "2012-05-20T09:15:30-08:00"
    }, 
    {
        "label": "RUNNING", 
        "t1": "2012-05-16T09:15:30-08:00", 
        "t2": "2012-05-20T09:18:30-08:00"
    }, 
    {
        "label": "STANDING", 
        "t1": "2012-05-16T09:18:30-08:00", 
        "t2": "2012-05-20T09:20:00-08:00"
    }
]
</code></pre>

<p><a id="test_results"></p>

<h2>Test Results</h2>

<p>The <a href="#recognizer_results">recognizer results</a> are combined with the <a href="#ground_truth">ground truth</a> items to encode the test results output. Results for the above example would be:</p>

<pre><code>{
    "description": "Activity recognition data collection with subject DR on morning of 5/16/2012", 
    "device": "Nokia C7-00", 
    "sw": "qt-hubris-client-v0.0.3", 
    "data_path": "raw_data/device1_20120516", 
    "t1": "2012-05-16T09:00:00-08:00"
    "t2": "2012-05-16T09:20:00-08:00", 
    "hw": "353755043225509", 
    "subject": {
        "weight": "165lbs", 
        "bday": "08/22/1975", 
        "gender": "male", 
        "handedness": "RIGHT", 
        "height": "183cm", 
        "id": "DR"
    }, 
    "labels": [
        {
            "t1": "2012-05-16T09:00:00-08:00", 
            "t2": "2012-05-20T09:05:00-08:00", 
            "label": "STANDING", 
            "body_position": "RIGHT_FRONT_POCKET"
        }, 
        {
            "t1": "2012-05-16T09:05:00-08:00", 
            "t2": "2012-05-20T09:15:00-08:00", 
            "label": "WALKING", 
            "body_position": "RIGHT_FRONT_POCKET"
        }, 
        {
            "t1": "2012-05-16T09:15:00-08:00", 
            "t2": "2012-05-20T09:15:30-08:00", 
            "label": "STANDING", 
            "body_position": "RIGHT_HAND"
        }, 
        {
            "t1": "2012-05-16T09:15:30-08:00", 
            "t2": "2012-05-20T09:18:30-08:00", 
            "label": "RUNNING", 
            "body_position": "RIGHT_BACK_POCKET"
        }, 
        {
            "t1": "2012-05-16T09:18:30-08:00", 
            "t2": "2012-05-20T09:20:00-08:00", 
            "label": "STANDING", 
            "body_position": "RIGHT_HAND"
        }
    ], 
    "results": [
        {
            "t1": "2012-05-16T09:00:00-08:00", 
            "t2": "2012-05-20T09:05:00-08:00", 
            "label": "STANDING"
        }, 
        {
            "t1": "2012-05-16T09:05:00-08:00", 
            "t2": "2012-05-20T09:15:00-08:00", 
            "label": "WALKING"
        }, 
        {
            "t1": "2012-05-16T09:15:00-08:00", 
            "t2": "2012-05-20T09:15:30-08:00", 
            "label": "STANDING"
        }, 
        {
            "t1": "2012-05-16T09:15:30-08:00", 
            "t2": "2012-05-20T09:18:30-08:00", 
            "label": "RUNNING"
        }, 
        {
            "t1": "2012-05-16T09:18:30-08:00", 
            "t2": "2012-05-20T09:20:00-08:00", 
            "label": "STANDING"
        }
    ], 
}
</code></pre>

<h2>References</h2>

<p><a name="ref1"/>
[1] Ward, J. A., Lukowicz, P., &amp; Gellersen, H. W. (2011). Performance metrics for activity recognition. ACM Transactions on Intelligent Systems and Technology (TIST), 2(1), 1-23. doi:10.1145/1889681.1889687. <a href="http://gtubicomp.pbworks.com/w/file/fetch/48480476/Ward2011-Performance%20metrics%20for%20activity%20recognition.pdf">PDF</a></p>
